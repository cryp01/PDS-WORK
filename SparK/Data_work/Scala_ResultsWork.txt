
[cloudera@quickstart spark]$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/flume-ng/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/parquet/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/avro/avro-tools-1.7.6-cdh5.12.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)
Type in expressions to have them evaluated.
Type :help for more information.
18/01/20 09:47:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context available as sc (master = local[*], app id = local-1516470448542).
18/01/20 09:48:16 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
SQL context available as sqlContext.

scala> val data = 1 to 100
data: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)

scala> val data=1 to 1000
data: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170...
scala> data
res0: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170...
scala> val distData = sc.parallelize(data)
distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29

scala> distData.filter(_< 100).collect()
res1: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)
scala> distData.filter(_> 100).collect()
res2: Array[Int] = Array(101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 25...
scala> distData.filter(_< 100).collect()
res3: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)

//Counting words with Spark (Scala)

scala> val f = sc.textFile("/hduser/input/2000.txt")
f: org.apache.spark.rdd.RDD[String] = /hduser/input/2000.txt MapPartitionsRDD[30] at textFile at <console>:27

scala> val wc = f.flatMap(l => l.split(" ")).
     | map(word => (word, 1)).
     | reduceByKey(_ + _)
wc: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[33] at reduceByKey at <console>:31

scala> wc.saveAsTextFile("wc2_out")

scala> exit

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/wc2_out
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2018-01-20 15:04 /user/cloudera/wc2_out/_SUCCESS
-rw-r--r--   1 cloudera cloudera     529020 2018-01-20 15:04 /user/cloudera/wc2_out/part-00000

[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/wc2_out/part-00000
(decilla,1)
(industriado,2)
(tratado;,1)
(altillo,1)
(moliesen,1)
(agilítanse,1)
(góticas,,1)
(todos,,63)
(veces!,1)
(avellanas,3)
(desgajar,1)
(giganta,2)
(mostréis,2)
(caló,1)
(hermosísima,,1)
(cortesía;,3)
(¡el,1)
(prevenirse,2)
(referirlas.,1)
((de,2)
(ciencias,,3)
(descrito,1)
(cayó,,2)
(Quiso,,1)
(proceder,6)
(montera,5)
(bárbaros,2)
(Desesperábase,3)
(confortativo,1)
(averiguado,,1)
(gatos,,4)
(desconocido,,1)
(parezcas,2)
(cena,,5)
(mal;,1)
(diga.,4)
(1.F.3.,1)
(experiencia,26)
(acabe?,1)
(caballos,13)
(hombruno;,1)
(Forte,,1)
(decirse,,8)
(firmas,3)
(-Pues,86)
(semanas,,1)
(entienda:,2)
(hablador,6)
(vencerlos,2)
(caballero,379)
(defraude,1)
(ahijársele,1)
(Ginés-,,2)
(creyente,,2)
(mansamente,,1)
(tragedia,,2)
(Cuelga,1)
(altar,,1)
(triste,38)
(cola,13)
(estimarse.,1)
(zabullían,1)
(adelanta,1)
(»Fue,1)
(subiendo,5)
(enderezar,,1)
(tal,,39)
(tuvímoslo,2)
(determinarse,3)
(granjeadas,1)
(desviada,2)
(borrarán,1)
(deshacedor,1)
(confírmole,1)
(dieron;,2)
(letra:,1)
(único,,1)
(continua,7)
(trasladaba,1)
(asesor.,1)
(falsos,2)
(Vandalia;,1)
(Dime,,8)
(resta,3)
(faltóles,1)
(ensartados.,1)
(padrino,2)
(Public,1)
(hacerte!,1)
(precedido,2)
(divinas.,1)
(hormiga,,1)
(baño,,7)
(artificioso,1)
(quimeras,,1)
(estienden,3)
(cordura,,2)
(adulación,,1)
(cuál,37)
(envuelta,2)
(cuadrillas,1)
(procurarlo,,1)
(''Señor,,2)
(sabrá,15)
(editions,,1)
(entenderos:,1)
(entrada,,2)
(serviros,,1)
(-Sepa,,1)
(refrán,,2)
(-Quisiera,4)
(Trapisonda;,1)
(escaparía.,1)
(desembarcado,2)
(digo,,87)
(Amiga,1)
(peinarse,1)
(encarecidamente,7)
(escrito;,1)
(dignos,9)
(punto.,15)
(cabrito,1)
(conocimiento;,1)
(aplacaron,1)
(matan.,1)
(celada,14)
(azul,2)
(acompañarle.,1)
(toda.,3)
(finojos,1)
(Suma,1)
(pasase,14)
(nudo,5)
(venzo,1)
(sacaremos,1)
(carnes?,2)
(haciéndose,5)
(vejez,4)
(-Muchacho,,1)
(acontecer,4)
(animal!,,1)
(menoscabo,10)
(Yace,2)
(cuello,15)
(concediese,,1)
(empreñar,,1)
(juicios;,1)
(apunté,1)
(felicidad.,1)
(Gaula?;,1)
(gustes,1)
(pareció;,2)
(cobraron,1)
(acometido,3)
(Vecinguerra,,1)
(orondas,2)
(recibí,,1)
(Llana.,2)
(forzaba,1)
(gana.,6)
(seso;,1)
(intricada,2)
(acaballa?,1)
(puntas,13)
(cabrero.,1)
(ello,63)
(brillando,1)
(computers.,1)
(ignoran.,1)
(Y,,312)
(maltrataban,,1)
(alguacil,,1)
(concavidad,,1)
(besarse.,1)
(adelantóse,1)
(encanto?,1)
(cubriese,2)
(capitán,,15)
(hagan.,1)
(antecedente,,1)
(vienen,33)
(leía,4)
(mensajero-;,1)
(acabase.,1)
(exageración,,1)
(discretos,,6)
(redropelo,1)
(desnudarse?,1)
(libre.,2)
(tardar,1)
(descubriésemos,1)
(estampado,1)
(corazón.,2)
(manjares;,1)
(maravillados,,1)
(agradecida,,2)
(muriere,,2)
(erizarse,1)
(palos;,1)
(pretenden,2)
(banda,,2)
(pollinos.,1)
(supiere,,1)
(bramido,,1)
(llama?,1)
(ahondar,1)
(brindaba,1)
(peloteando,1)
(Benito,,2)
(oía,,4)
(Italia:,1)
(»Estaban,1)
(pecadoras,1)
(vía.,1)
(promete,3)
(rúbrica,,1)
(coser,3)
(trasnochar,1)
(arise,1)
(aciertan,2)
(cordellate;,1)
(desvióse,1)
(¿cuál,4)
(jardín,21)
(divierto?,1)
(ninguno,,12)
(tomaron,8)
(marfil,1)
(servicios!,1)
(encarecimiento,2)
(correspondes,1)
(than,2)
(asados,1)
(anochece,1)
(apareja,1)
(forzó,4)
(estraordinario,2)
(valiente;,1)
(aspa,,1)
(Leonela,22)
(ginovesa,1)
(limpia,,5)
(movible,2)
(desprecio,2)
(iban,45)
(cuello,,16)
(antojos,,1)
(fuere,,6)
(durar,3)
(florido,1)
(located,4)
(escudero,,78)
(escote,,1)
(francés,,1)
(empozarme,1)
(adonde,,3)
(discurrido,1)
(devotamente,1)
(catalana,1)
(Faldas;,1)
(hermosos,,1)
(colchones,3)
(Soberana,1)
(flacas,2)
(cabo,79)
(culpa,38)
(nonproprietary,1)
(perdió,,2)
(determinaron,11)
(interromperéis,1)
(atrás;,1)
(verdades,12)
(lagartos,1)
(mentecato?;,1)
(mostraron,,1)
(otras.,3)
(marchita,1)
(regaladamente,1)
(limosna.,1)
(caballería!,2)
(pretende,4)
(erario,1)
(propio,,4)
(cabrón.,1)
(honradas,,1)
(-Sí,40)
(leyóle,1)
(halláronle,2)
(gastaba,1)
(roca,1)
(Tibre?,1)
(dellos",,1)
(encamine.,1)
(Venus,2)
(levantada,,3)
(escuras,,2)
(¡No,,10)
(adorar,2)
(bastaba,8)
(estado,,20)
(convenirle,1)
(inacabable,1)
(hueso!,1)
(reírse,3)
(cumple,4)
(olores;,1)
(consagro,,1)
(respondelle,,1)
(siéndolo,4)
(peligrosos;,1)
(áspera;,1)
(acogidos,1)
(menor,15)
(sentirle,1)
(escuchan,2)
(corde,1)
(-O,3)
(parar,31)
(alcahuete,,4)
(arrieros,7)
(mina,,1)
(aumentarán,1)
(amparar,3)
(coyuntura,,4)
(pestífera,,1)
(quedéis,3)
(desnuda,2)
(»Hasta,1)
(algo,,14)
(confirme,1)
(fidelísima,,1)
(sobresaltos,3)
(canto.,1)
(secretas.,1)
(envidiara,,1)
(entregará,1)
(poderla,1)
(nueve,5)
(fru-,,1)
(esquilón,1)
(rasgados,,1)
(fínisimo,1)
(posesión!,1)
(guíes,1)
(sustenta;,2)
(más,1823)
(desamorado,,2)
(miro,1)
(sofistería,1)
(redunde.,1)
(bélico,1)
(libraría,,1)
(entreoyóle,1)
(bellaquería;,1)
(parecerle,17)
(resolverse,1)
(afrenta,10)
(compo-,,1)
(dueño,27)
(carrillo,,1)
(understand,,1)
(cerrase.,1)
(armada.,2)
(DE,31)
(enredarse,1)
(acompañamiento,,3)
(sedero,,1)
(ayuda?,1)
(eran;,1)
(sagaz,2)
(quiebre,1)
(India,,1)
(hazañas,,13)
(tratare,1)
(muertos,,5)
(bandera,3)
(creídas.,1)
(entrándose,1)
(Sintió,2)
(preguntado:,1)
(cevil.,1)
(matase,,1)
(-Deténgome,1)
(remitieron,1)
(credos,,2)
(vendían;,1)
(breve.,1)
(sentenciado.,1)
(Cicerón;,1)
(heridas,,5)
(deshonra;,1)
(distributed:,1)
(enamorado:,1)
(contáronle,1)
(regía,,1)
(conociésedes,1)
(ordena,,1)
(desviarse,,1)
(salterios,,1)
(paraba,12)
(ofreció,26)
(llovido,,1)
(descuido,13)
(ido.,2)
(industriándole,1)
(descompuestos,,1)
(preciaban,1)
(ocultarlos.,1)
(-o,1)
(cuñado,1)
(traza,25)
(él,,168)
(impertinencias.,1)
(combaten,1)
(siga,1)
(inhumanidad,1)
(Roldanes.,1)
(arranca,1)
(osara,1)
(encargase;,1)
(quedé,10)
(cifra,4)
(vistiéndonos,1)
(sabia,3)
(ojeriza,,2)
(llaméis,1)
(apellidarían,1)
(Colgó,1)
(vellas,1)
(caballeresca,,1)
(francés,3)
(avellanadas,,1)
(-Pintad,1)
(opinión;,1)
(componía,2)
(mirábase,1)
(pensase,7)
(desgajó,3)
(verá,39)
(juntos?,1)
(imprudente,1)
(doctrínala,1)
(encasquetóse,1)
(borrarle,1)
(lot,1)
(dejen,9)
(estorbársela,,1)
(lastimados,2)
(altaneros;,1)
(Pedro;,1)
(Quéjese,1)
(-Escríbala,1)
(ofrecerle,,1)
(prestado,6)
(sea,223)
(aragoneses,,1)
(estaréis,1)
(Yo,,26)
(vemos,4)
(litera,,1)
(decentes,,1)
(lloramicos,,1)
(mejoran,1)
(juré,1)
(amenaza;,1)
(partirse,,2)
(valores,1)
(azotar,2)
(eres",,1)
(dar,,14)
(mentía,,1)
(casárase,1)
(Bretaña.,1)
(confíese,1)
(intitulada,1)
(cantar,,3)
(interviene,1)
(codicio,1)
(tratare!,1)
(celebérrimo!,1)
(imposible,26)
(risueña.,1)
(gobernare.,1)
(viejo-.,1)
(besamos,1)
(júzguenlo,1)
(cibera,,1)
(vengas,3)
(Lorenzo:,3)
(albricias,3)
(maldiciente,2)
(retirada,,3)
(memorable,5)
(permitía.,1)
(misericordia,,5)
(vine,8)
(especias,1)
(bigotes,4)
(está,415)
(discreta.,1)
(mansa:,1)
(hidalguía,1)
(otorgaba,1)
(fatigaba,,1)
(Urganda,,2)
(longura,1)
(díganme:,2)
(rodea!,1)
(arco,3)
(ál,,1)
(alcanzar,,2)

// Type and understand the following source code in Spark console. 
// Upload reg.tsv and clk.tsv at your own HDFS data input folder.

scala> val format = new java.text.SimpleDateFormat("yyyy-MM-dd")
format: java.text.SimpleDateFormat = java.text.SimpleDateFormat@f67a0200

scala> case class Register (d: java.util.Date, uuid: String, cust_id:
     | String, lat: Float, lng: Float)
defined class Register

scala> case class Click (d: java.util.Date, uuid: String, landing_page:
     | Int)
defined class Click

scala> val reg = sc.textFile("/hduser/input/reg.tsv").map(_.split("\t")).
     | map( r => (r(1),
     | Register(format.parse(r(0)),r(1),r(2),r(3).toFloat,
     | r(4).toFloat))
     | )
reg: org.apache.spark.rdd.RDD[(String, Register)] = MapPartitionsRDD[44] at map at <console>:32

scala> val clk = sc.textFile("/hduser/input/clk.tsv").map(_.split("\t")).
     | map(c => (c(1),Click(format.parse(c(0)),c(1),
     | c(2).trim.toInt))
     | )
clk: org.apache.spark.rdd.RDD[(String, Click)] = MapPartitionsRDD[48] at map at <console>:32

scala> reg.join(clk).collect()
18/01/20 16:44:02 ERROR executor.Executor: Exception in task 0.0 in stage 4.0 (TID 4)
java.lang.ArrayIndexOutOfBoundsException: 1
	at $line57.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at $line57.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
18/01/20 16:44:02 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at $line57.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at $line57.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

18/01/20 16:44:02 ERROR scheduler.TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
18/01/20 16:44:02 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, localhost, executor driver): TaskKilled (killed intentionally)
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1862)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1875)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1888)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1959)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:43)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:45)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:47)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:49)
	at $iwC$$iwC$$iwC.<init>(<console>:51)
	at $iwC$$iwC.<init>(<console>:53)
	at $iwC.<init>(<console>:55)
	at <init>(<console>:57)
	at .<init>(<console>:61)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1045)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1326)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:821)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:800)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1064)
	at org.apache.spark.repl.Main$.main(Main.scala:35)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:32)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)


scala> reg.join(clk).toDebugString
res11: String = 
(1) MapPartitionsRDD[54] at join at <console>:38 []
 |  MapPartitionsRDD[53] at join at <console>:38 []
 |  CoGroupedRDD[52] at join at <console>:38 []
 +-(1) MapPartitionsRDD[44] at map at <console>:32 []
 |  |  MapPartitionsRDD[43] at map at <console>:31 []
 |  |  /hduser/input/reg.tsv MapPartitionsRDD[42] at textFile at <console>:31 []
 |  |  /hduser/input/reg.tsv HadoopRDD[41] at textFile at <console>:31 []
 +-(1) MapPartitionsRDD[48] at map at <console>:32 []
    |  MapPartitionsRDD[47] at map at <console>:31 []
    |  /hduser/input/clk.tsv MapPartitionsRDD[46] at textFile at <console>:31 []
    |  /hduser/input/clk.tsv HadoopRDD[45] at textFile at <console>:31 []

//Resilient Distributed Datasets (RDD) 

scala> val data = Array(1, 2, 3, 4, 5)
data: Array[Int] = Array(1, 2, 3, 4, 5)

scala> val distData = sc.parallelize(data)
distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[55] at parallelize at <console>:29

scala> val distFile = sc.textFile("/hduser/input/2000.txt")
distFile: org.apache.spark.rdd.RDD[String] = /hduser/input/2000.txt MapPartitionsRDD[64] at textFile at <console>:27

scala> distFile.map(l => l.split(" ")).collect()
res13: Array[Array[String]] = Array(Array(The, Project, Gutenberg, EBook, of, Don, Quijote,, by, Miguel, de, Cervantes, Saavedra), Array(""), Array(This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with), Array(almost, no, restrictions, whatsoever., "", You, may, copy, it,, give, it, away, or), Array(re-use, it, under, the, terms, of, the, Project, Gutenberg, License, included), Array(with, this, eBook, or, online, at, www.gutenberg.net), Array(""), Array(""), Array(Title:, Don, Quijote), Array(""), Array(Author:, Miguel, de, Cervantes, Saavedra), Array(""), Array(Posting, Date:, April, 27,, 2010, [EBook, #2000]), Array(Release, Date:, December,, 1999), Array(""), Array(Language:, Spanish), Array(""), Array(""), Array(***, START, OF, THIS, PROJECT, GUTENBERG, EBOO...

scala> distFile.map(l => l.split(" ")).collect()
res13: Array[Array[String]] = Array(Array(The, Project, Gutenberg, EBook, of, Don, Quijote,, by, Miguel, de, Cervantes, Saavedra), Array(""), Array(This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with), Array(almost, no, restrictions, whatsoever., "", You, may, copy, it,, give, it, away, or), Array(re-use, it, under, the, terms, of, the, Project, Gutenberg, License, included), Array(with, this, eBook, or, online, at, www.gutenberg.net), Array(""), Array(""), Array(Title:, Don, Quijote), Array(""), Array(Author:, Miguel, de, Cervantes, Saavedra), Array(""), Array(Posting, Date:, April, 27,, 2010, [EBook, #2000]), Array(Release, Date:, December,, 1999), Array(""), Array(Language:, Spanish), Array(""), Array(""), Array(***, START, OF, THIS, PROJECT, GUTENBERG, EBOO...
scala> distFile.flatMap(l => l.split(" ")).collect()
res14: Array[String] = Array(The, Project, Gutenberg, EBook, of, Don, Quijote,, by, Miguel, de, Cervantes, Saavedra, "", This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with, almost, no, restrictions, whatsoever., "", You, may, copy, it,, give, it, away, or, re-use, it, under, the, terms, of, the, Project, Gutenberg, License, included, with, this, eBook, or, online, at, www.gutenberg.net, "", "", Title:, Don, Quijote, "", Author:, Miguel, de, Cervantes, Saavedra, "", Posting, Date:, April, 27,, 2010, [EBook, #2000], Release, Date:, December,, 1999, "", Language:, Spanish, "", "", ***, START, OF, THIS, PROJECT, GUTENBERG, EBOOK, DON, QUIJOTE, ***, "", "", "", "", Produced, by, an, anonymous, Project, Gutenberg, volunteer., Text, file, corrections, and, new, HTML,...



